{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd08f7a9caefe3fbe01c4aa0205e6cc1c36763ed3d41b234746df90d0099aaa4109",
   "display_name": "Python 3.8.8 64-bit ('tf': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import tensorflow_datasets as tensorflow_datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, TFT5ForConditionalGeneration\n",
    "import pickle as pkl\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='./data'\n",
    "log_dir='./logs'\n",
    "save_path='./models'\n",
    "tokenizer_path='cache/t5-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset squad (./data\\squad\\plain_text\\1.0.0\\4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\n",
      "Reusing dataset squad (./data\\squad\\plain_text\\1.0.0\\4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\n"
     ]
    }
   ],
   "source": [
    "train=load_dataset('squad',split='train',cache_dir=data_dir)\n",
    "val=load_dataset('squad',split='validation',cache_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WARMUP_STEPS=1e4\n",
    "BATCH_SIZE=4\n",
    "ENCODER_MAXLEN=250\n",
    "DECODER_MAX_LEN=75\n",
    "BUFFER_SIZE=1000\n",
    "LEN_TRAIN=len(train)\n",
    "LEN_VAL=len(val)\n",
    "TRAIN_STEPS=int(np.ceil(LEN_TRAIN/BATCH_SIZE))\n",
    "VAL_STEPS=int(np.ceil(LEN_VAL/BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(instance,encoder_maxlen=ENCODER_MAXLEN,decoder_maxlen=DECODER_MAX_LEN):\n",
    "    context=instance['context']\n",
    "    question=instance['question']\n",
    "    answers=instance['answers']['text']\n",
    "    \n",
    "    new_question=f'question: {str(question)} context: {str(context)} </s>'\n",
    "    new_answers=', '.join([answer for answer in list(answers)])\n",
    "    new_answers=f'{new_answers} <\\s>'\n",
    "\n",
    "    encoder_inputs=tokenizer(new_question,truncation=True,return_tensors='tf',max_length=encoder_maxlen,pad_to_max_length=True)\n",
    "    decoder_inputs=tokenizer(new_answers,truncation=True,return_tensors='tf',max_length=decoder_maxlen,pad_to_max_length=True)\n",
    "\n",
    "    input_ids=encoder_inputs['input_ids'][0]\n",
    "    input_attention=encoder_inputs['attention_mask'][0]\n",
    "    target_ids=decoder_inputs['input_ids'][0]\n",
    "    target_attention=decoder_inputs['attention_mask'][0]\n",
    "\n",
    "    outputs={\n",
    "        'input_ids':input_ids,\n",
    "        'attention_mask':input_attention,\n",
    "        'labels':target_ids,\n",
    "        'decoder_attention_mask':target_attention\n",
    "    }\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/87599 [00:00<?, ?ex/s]F:\\anacondapython\\envs\\tf\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2104: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 87599/87599 [02:35<00:00, 563.60ex/s]\n",
      "100%|██████████| 10570/10570 [00:19<00:00, 556.05ex/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds=train.map(encode)\n",
    "val_ds=val.map(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.save_to_disk('datasets/train_ds')\n",
    "val_ds.save_to_disk('datasets/val_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}