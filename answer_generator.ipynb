{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd08f7a9caefe3fbe01c4aa0205e6cc1c36763ed3d41b234746df90d0099aaa4109",
   "display_name": "Python 3.8.8 64-bit ('tf': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import tensorflow_datasets as tensorflow_datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, TFT5ForConditionalGeneration\n",
    "import pickle as pkl\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='./data'\n",
    "log_dir='./logs'\n",
    "save_path='./models'\n",
    "tokenizer_path='cache/t5-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset squad (./data\\squad\\plain_text\\1.0.0\\4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\n",
      "Reusing dataset squad (./data\\squad\\plain_text\\1.0.0\\4fffa6cf76083860f85fa83486ec3028e7e32c342c218ff2a620fc6b2868483a)\n"
     ]
    }
   ],
   "source": [
    "train=load_dataset('squad',split='train',cache_dir=data_dir)\n",
    "val=load_dataset('squad',split='validation',cache_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WARMUP_STEPS=1e4\n",
    "BATCH_SIZE=4\n",
    "ENCODER_MAXLEN=250\n",
    "DECODER_MAX_LEN=75\n",
    "BUFFER_SIZE=1000\n",
    "LEN_TRAIN=len(train)\n",
    "LEN_VAL=len(val)\n",
    "TRAIN_STEPS=int(np.ceil(LEN_TRAIN/BATCH_SIZE))\n",
    "VAL_STEPS=int(np.ceil(LEN_VAL/BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(instance,encoder_maxlen=ENCODER_MAXLEN,decoder_maxlen=DECODER_MAX_LEN):\n",
    "    context=instance['context']\n",
    "    question=instance['question']\n",
    "    answers=instance['answers']['text']\n",
    "    \n",
    "    new_question=f'question: {str(question)} context: {str(context)} </s>'\n",
    "    new_answers=', '.join([answer for answer in list(answers)])\n",
    "    new_answers=f'{new_answers} <\\s>'\n",
    "\n",
    "    encoder_inputs=tokenizer(new_question,truncation=True,return_tensors='tf',max_length=encoder_maxlen,pad_to_max_length=True)\n",
    "    decoder_inputs=tokenizer(new_answers,truncation=True,return_tensors='tf',max_length=decoder_maxlen,pad_to_max_length=True)\n",
    "\n",
    "    input_ids=encoder_inputs['input_ids'][0]\n",
    "    input_attention=encoder_inputs['attention_mask'][0]\n",
    "    target_ids=decoder_inputs['input_ids'][0]\n",
    "    target_attention=decoder_inputs['attention_mask'][0]\n",
    "\n",
    "    outputs={\n",
    "        'input_ids':input_ids,\n",
    "        'attention_mask':input_attention,\n",
    "        'labels':target_ids,\n",
    "        'decoder_attention_mask':target_attention\n",
    "    }\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/87599 [00:00<?, ?ex/s]F:\\anacondapython\\envs\\tf\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2104: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 87599/87599 [02:35<00:00, 563.60ex/s]\n",
      "100%|██████████| 10570/10570 [00:19<00:00, 556.05ex/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds=train.map(encode)\n",
    "val_ds=val.map(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.save_to_disk('datasets/train_ds')\n",
    "val_ds.save_to_disk('datasets/val_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=datasets.load_from_disk('datasets/train_ds')\n",
    "val_ds=datasets.load_from_disk('datasets/val_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tf_dataset(dataset):\n",
    "    cols=['input_ids','attention_mask','labels','decoder_attention_mask']\n",
    "    dataset.set_format(type='tensorflow',columns=cols)\n",
    "    return_types={\n",
    "        'input_ids':tf.int32,\n",
    "        'attention_mask':tf.int32,\n",
    "        'labels':tf.int32,\n",
    "        'decoder_attention_mask':tf.int32,\n",
    "    }\n",
    "    return_shapes={\n",
    "        'input_ids':tf.TensorShape([None]),\n",
    "        'attention_mask':tf.TensorShape([None]),\n",
    "        'labels':tf.TensorShape([None]),\n",
    "        'decoder_attention_mask':tf.TensorShape([None]),\n",
    "    }\n",
    "    ds=tf.data.Dataset.from_generator(lambda:dataset,return_types,return_shapes)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_ds=convert_to_tf_dataset(train_ds)\n",
    "tf_val_ds=convert_to_tf_dataset(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(tf_train_ds,'datasets/tf_train_ds')\n",
    "tf.data.experimental.save(tf_val_ds,'datasets/tf_val_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ready_ds(ds,batch_size=BATCH_SIZE,buffer_size=BUFFER_SIZE):\n",
    "    ds=ds.shuffle(buffer_size).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_ds=ready_ds(tf_train_ds)\n",
    "final_val_ds=ready_ds(tf_val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Model(TFT5ForConditionalGeneration):\n",
    "\n",
    "    def __init__(self,*args,log_dir=None,cache_dir=None,**kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        self.loss_tracker=tf.keras.metrics.Mean(name='loss')\n",
    "    \n",
    "    # @tf.function\n",
    "    def train_step(self,data):\n",
    "        x=data\n",
    "        y=tf.reshape(x['labels'],[-1,1])\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs=self(x,training=True)\n",
    "            logits=outputs[1]\n",
    "            loss=tf.reduce_mean(outputs[0])\n",
    "            grads=tape.gradient(loss,self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads,self.trainable_variables))\n",
    "        lr=self.optimizer._decayed_lr(tf.float32)\n",
    "        self.loss_tracker.update_state(y,loss)\n",
    "        self.compiled_metrics.update_state(y,logits)\n",
    "        metrics={m.name:m.result() for m in self.metrics}\n",
    "        metrics.update({'lr':lr})\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self,data):\n",
    "        x=data\n",
    "        y=tf.reshape(x['labels'],[-1,1])\n",
    "        outputs=self(x,training=False)\n",
    "        loss=tf.reduce_mean(outputs[0])\n",
    "        logits=outputs[1]\n",
    "        self.loss_tracker.update_state(y,loss)\n",
    "        self.compiled_metrics.update_state(y,logits)\n",
    "        metrics={m.name:m.result() for m in self.metrics}\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLrSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self,warmup_steps=WARMUP_STEPS):\n",
    "        super().__init__()\n",
    "        self.warmup_steps=tf.cast(warmup_steps,tf.float32)\n",
    "    \n",
    "    def __call__(self,step):\n",
    "        step=tf.cast(step,tf.float32)\n",
    "        m=tf.cast(tf.maximum(self.warmup_steps,step),tf.float32)\n",
    "        lr=tf.math.rsqrt(m)\n",
    "        return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_profile_batch=TRAIN_STEPS+10\n",
    "stop_profile_batch=start_profile_batch+100\n",
    "profile_range=f'{start_profile_batch},{stop_profile_batch}'\n",
    "\n",
    "log_path=log_dir\n",
    "tensorboard_callback=[tf.keras.callbacks.TensorBoard(log_dir=log_path,histogram_freq=1,update_freq=20,profile_batch=profile_range)]\n",
    "\n",
    "checkpoint_path=save_path+'/'+'T5.h5'\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(save_path,monitor='val_loss',save_best_only=True)\n",
    "\n",
    "callbacks=[tensorboard_callback,checkpoint_callback]\n",
    "metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy(name='accuracy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=CustomLrSchedule()\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "All model checkpoint layers were used when initializing T5Model.\n",
      "\n",
      "All the layers of T5Model were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5Model for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model=T5Model.from_pretrained('t5-base',cache_dir='model_cache/t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 14956."
     },
     "metadata": {}
    }
   ],
   "source": [
    "%tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(final_train_ds,epochs=5,steps_per_epoch=TRAIN_STEPS,callbacks=callbacks,validation_data=final_val_ds,validation_steps=VAL_STEPS,initial_epoch=0)"
   ]
  }
 ]
}